<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset = "UTF-8">
        <meta name = "viewport" content = "width=device-wideth, initial-scale=1.0">
        <title>Session 5</title>

        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,900|
        Source+Sans+Pro:300,900&display=swap">


        <link rel="stylesheet" href="css/courses.css">



    </head>

    <body>



        <header>

            <div class="rectangle">
              <button type="button" class="home_button" onClick="parent.location='index.html'">Home</button>
                <button type="button" class="prev_button" onClick="parent.location='s4.html'">Previous</button>
                <button type="button" class="next_button" onClick="parent.location='s6.html'">Next</button>
            </div>





        </header>


        <div class="content">


        <div class="content_list">

          <ul class="collapsibleList">
              <li class="parent-item">
                <a href="exercises.html">Machine Learning Exercises</a>
              </li>
              <li class="parent-item">
                <a href="s1.html">Session 1: Introduction to Machine Learning</a>
                <ul>
                  <li>Introduction to Machine Learning</li>
                  <li>What is machine learning</li>
                  <li>Influences of Machine Learning</li>
                  <li>Components of Machine Learning</li>
                    <ul>
                      <li>Representation</li>
                      <li>Evaluation</li>
                      <li>Optimization</li>
                    </ul>
                  <li>Types of Machine Learning</li>
                  <li>Challenges of Machine Learning</li>
                  <li>Before starting your own machine learning project</li>
                </ul>
              </li>

              <li class="parent-item">
                  <a href="s2.html">Session 2: Bayesian Decision Theory</a>
                  <ul>
                    <li>What is a Bayesian network?</li>
                    <li>How is a Bayesian network represented?</li>
                    <li>What Directed Acyclic Graphs Tell Us</li>
                    <li>Applications of Bayesian Networks</li>
                  </ul>
              </li>

              <li class="parent-item">
                <a href="s3.html">Session 3: Intro to Supervised Learning</a>
                <ul>
                  <li>Intro to Supervised Learning</li>
                  <li>When to use supervised learning</li>
                  <li>Types of Supervised Learning</li>
                    <ul>
                      <li>Classification</li>
                      <li>Regression</li>
                    </ul>
                  <li>Examples of Supervised Learning Algorithms</li>
                    <ul>
                      <li>Linear Regression</li>
                      <li>Logistic Regression</li>
                      <li>Decision Trees</li>
                      <li>Random Forest Regression</li>
                    </ul>
                </ul>
              </li>

              <li class="parent-item">
                <a href="s4.html">Session 4: Intro to Unsupervised Learning</a>
                <ul>
                  <li>When to use unsupervised learning</li>
                  <li>Types of Unsupervised Learning</li>
                    <ul>
                      <li>Clustering</li>
                      <li>Association</li>
                    </ul>
                  <li>Example of an Unsupervised Learning Algorithm</li>
                    <ul>
                      <li>K-Means Clustering</li>
                    </ul>
                </ul>
              </li>

              <li class="parent-item">
                <a href="s5.html">Session 5: Dimensionality Reduction</a>
                <ul>
                  <li>Intro to Dimensionality Reduction</li>
                  <li>Feature Selection</li>
                  <li>Types of Feature Selection</li>
                    <ul>
                      <li>Wrappers</li>
                      <li>Filters</li>
                      <li>Embedded</li>
                    </ul>
                  <li>Feature Extraction</li>
                  <li>Example of A Feature Extraction Technique</li>
                    <ul>
                      <li>Principal Component Analysis</li>
                    </ul>
                </ul>
              </li>

              <li class="parent-item">
                <a href="s6.html">Session 6: Perceptrons and Neural Networks</a>
                <ul>
                  <li>Influence of Biological Neurons</li>
                  <li>Development of Neural Networks</li>
                  <li>Perceptron</li>
                    <ul>
                      <li>What is a perceptron?</li>
                      <li>Parts of Perceptron</li>
                    </ul>
                  <li>Neural Network</li>
                </ul>
              </li>

              <li class="parent-item">
                <a href="s7.html">Session 7: Deep Learning</a>
                <ul>
                  <li>Deep Learning vs Machine Learning</li>
                  <li>Deep Neural Network</li>
                  <li>Deep Learning</li>
                    <ul>
                      <li>Transfer Learning</li>
                    </ul>
                  <li>Number of Neurons for Each Hidden Layer</li>
                </ul>
              </li>

              <li class="parent-item">
                <a href="s8.html">Session 8: Machine Learning Research Trends</a>
                <ul>
                  <li>Applications of Machine Learning in Various Fields</li>
                  <li>Full-Stack Deep Learning</li>
                  <li>Natural Language Processing</li>
                  <li>Natural Disaster Warning</li>
                  <li>Gaming</li>
                  <li>Resources to Look into Latest Research</li>
                </ul>
              </li>
            </ul>
      </div>
      <!-- <script src="js/CollapsableList.js"></script> -->

      <div class="instruction">

        <h2>Session 5: Dimensionality Reduction</h2>
        <p> <mark>Intro to Dimensionality Reduction</mark></p>
          <ul>
          <li>When we have huge data, it takes a lot of resources to process
            all of it and we could even have millions of features for each
            instance. Sometimes, this data might even be completely unorganized
            and contain redundant or noisy features. This is where dimensionality
            reduction comes into play.</li>
          <li>Dimensionality Reduction improves performance by reducing the
            number of features. It can produce a better solution than if the
            data was just processed by itself.</li>
          <li>Dimensionality reduction also makes it easier to visualize data
            and identify clusters.</li>
          <li>It’s already pretty hard to visualize 4 dimensions, since we
            are used to a 3-dimensional world, so dimensionality reduction
            can help us see our data in fewer dimensions (even in 2 or 3 dimensions).
            In fact, the more dimensions we have, the more likely it’ll overfit our
            model and provide less accuracy.</li>
          <li>One approach of dimensionality reduction is with projection, for
            example, converting a 3D graph to 2D by projecting it onto a plane.</li>
          </ul>

        <p> <mark>Feature Selection</mark></p>
          <ul>
          <li>Feature selection only selects relevant variables and reduces
            the irrelevant variables. This will provide more accuracy and give
            better results since it ensures that only the necessary features are included.</li>
          <li>If a variable is a super small number that has no impact if taken
            out, then this variable can be eliminated.</li>
          </ul>

        <p> <mark>Types of Feature Selection</mark></p>
          <ul>
          <li>Wrappers</li>
            <ul>
            <li>The chosen feature selection algorithm acts as a wrapper around the
              predictive model, which goes through features individually, either adding
              it or deleting it.</li>
            <li>Out of the three types of feature selection, this is the most computationally
              intensive but will provide the best selection of features.</li>
            <li>Prone to overfitting.</li>
            <li>Examples:</li>
              <ul>
              <li>Forward stepwise regression</li>
              <li>Backward stepwise regression</li>
              </ul>
            </ul>
          <li>Filters
            <ul>
            <li>While this method filters out information that’s not needed
              using patterns such as correlation, it will give a good prediction,
              but won’t necessarily give the best outcome.</li>
            <li>No predictive model is used, so it is completely based on the
              relationships between the features. If features have a good relationship
              and raise accuracy, the feature will be kept but features that drop
              accuracy will be filtered out.</li>
            <li>Out of the three types of feature selection, this is the fastest
              and least computationally intensive, which is especially useful when
              there are a large amount of features.</li>
            <li>Less prone to overfitting</li>
            <li>Examples:</li>
              <ul>
              <li>Chi-Square Test</li>
              <li>Correlation</li>
              </ul>
            </ul>
          <li>Embedded</li>
            <ul>
            <li>This method is embedded during each step of the model
              building process and selects the best features that will make the
              most accurate model.</li>
            <li>Out of the three types of feature selection, this is right
              in the middle. It’s not as fast as the filter method but is
              faster than the wrapper method.</li>
            <li>Less prone to overfitting</li>
            <li>Example:</li>
              <ul>
              <li>Regularization model</li>
              </ul>
            </ul>
        </ul>

        <p> <mark>Feature Extraction</mark></p>
        <ul>
          <li>Feature extraction creates new features from existing features.</li>
          <li>It has the ability to represent data in fewer dimensions when it
            originally has multiple dimensions. Ultimately, the computational
            resources used are reduced.</li>
        </ul>

        <p> <mark>Example of A Feature Extraction Technique</mark></p>
          <ul>
          <li>Principal Component Analysis</li>
            <ul>
            <li>A linear technique that is commonly known as PCA.</li>
            <li>Using projection, PCA reduces the number of dimensions.
              Ideally, this number sums to at least 95% of variance. As
              it projects the components, new axes called principal axes
              are created to describe this new relationship. The axes are
              ranked in order of importance. Using Singular value decomposition
              which is essentially matrix factorization, it creates a new set of
              data known as the principal components.</li>
            <li>Ultimately, it compresses the dimensions and makes the data easier
              to work with.</li>
            <li>In cases like data visualization, where it is really hard to
              visualize more than 3 dimensions, PCA makes data easier to visualize
              by reducing it into 2 or 3 dimensions.</li>
            </ul>
          </ul>
      </div>

        </div>




    </body>

    <footer>


    </footer>




</html>
