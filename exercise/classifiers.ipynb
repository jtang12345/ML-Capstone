{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6uz-mgBihrz0"},"outputs":[],"source":["import numpy as np # importing NumPy\n","from sklearn import datasets\n","from sklearn.model_selection import KFold # importing KFold\n","from sklearn.linear_model import LogisticRegression # for Logistic Regression \n","from sklearn.naive_bayes import GaussianNB # for Naive Bayes\n","from sklearn.neighbors import KNeighborsClassifier # for kNN\n","from sklearn.svm import LinearSVC # for linear SVM\n","from sklearn import tree # for dicision tree\n","from sklearn.svm import SVC # for SVM with RBF kernel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r30-T9ymh2tb"},"outputs":[],"source":["X, y = datasets.load_breast_cancer(return_X_y=True)"]},{"cell_type":"markdown","metadata":{"id":"zW7e01eHjdr6"},"source":["Using k-folds we will 'fold' the data, essentially changing witch data instaces are test/train each loop and average the accuracies of each model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLwRb3hVjtL8"},"outputs":[],"source":["#intialize a list of lists to hold the accuracies\n","accuracies = [[] for _ in range(6)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhxIqPpvitHC"},"outputs":[],"source":["#Make kfolds\n","kf = KFold()\n","folds = kf.get_n_splits(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1898,"status":"ok","timestamp":1650048884640,"user":{"displayName":"Michael Brine","userId":"04374685200782582587"},"user_tz":420},"id":"VXwSqmRNi4iC","outputId":"771a18a3-f38f-4561-bfa4-0ea84f3026cb"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n"]}],"source":["#this will loop the above output times\n","# read the sklearn document to see what KFold.split returns\n","for train_i, test_i in kf.split(X):\n","    X_train, X_test = X[train_i], X[train_i]\n","    y_train, y_test = y[train_i], y[train_i]\n","\n","    #Logistic Regression\n","    [TODO]\n","\n","    # Naive Bayes\n","    [TODO]\n","\n","    # KNN\n","    [TODO]\n","    \n","\n","    # Linear SVM\n","    #linear_svm = # Use sklearn API for linear SVM (with no options)\n","    [TODO]\n","\n","    # Decision Tree\n","    [TODO]\n","\n","    # SVM with rbf kernel\n","    [TODO]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650048884641,"user":{"displayName":"Michael Brine","userId":"04374685200782582587"},"user_tz":420},"id":"_cYcajzRkMK5","outputId":"e6948804-4ba0-4e2e-e51b-7ba1bf765b28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average accuracies = [0.9547426257952573, 0.9428822055137844, 0.9455186042028147, 0.8950347021399654, 1.0, 0.9182803161750529]\n","Highest accuracy is 1.0 using Decision Tree\n"]}],"source":["#Average the accuracies over the # of folds\n","avg = [(sum(accuracies[x]) / folds) for x in range(6)]\n","print(\"Average accuracies = \" + str(avg))\n","print(\"Highest accuracy is \" + str(max(avg)) + \" using \", end=\"\")\n","winner = avg.index(max(avg))\n","if winner == 0:\n","    print(\"Logistic Regression\")\n","elif winner == 1:\n","    print(\"Naive Bayes\")\n","elif winner == 2:\n","    print(\"KNN\")\n","elif winner == 3:\n","    print(\"Linear SVM\")\n","elif winner == 4:\n","    print(\"Decision Tree\")    \n","elif winner == 5:\n","    print(\"SVM with RBF kernel\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1218,"status":"ok","timestamp":1650302771163,"user":{"displayName":"astrdino","userId":"16505414506703285777"},"user_tz":420},"id":"J1DrOry4tabc","outputId":"f71f5080-c746-4b37-9726-e7ce69b6ca3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[NbConvertApp] Converting notebook /content/nn_project_key.ipynb to html\n","[NbConvertApp] Writing 292921 bytes to /content/nn_project_key.html\n"]},{"data":{"text/plain":[]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","jupyter nbconvert --to html /content/nn_project_key.ipynb"]}],"metadata":{"colab":{"name":"classifiers.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
